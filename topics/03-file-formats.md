# Topic 3: Data File Formats

## ðŸŽ¯ Learning Goals

By the end of this topic, you should be able to:
- Understand key data file formats: ORC, Parquet, Avro, JSON, and CSV
- Explain **row-based vs column-based** storage and why **row = faster writes**, **column = faster reads**
- Describe **predicate pushdown**, **data skipping**, and **partition pruning** and how they improve query performance
- Compare **compression types** (e.g. Snappy, GZIP, LZ4) and when to use each
- Choose the right format for storage, streaming, analytics, and exchange
- Explain schema evolution support across formats
- Apply compression and performance trade-offs in practice

---

## ðŸ“– Core Concepts

### 1. Why File Formats Matter

The choice of file format significantly impacts:
- **Storage efficiency** (compression, size)
- **Query performance** (read/write speed, predicate pushdown)
- **Schema evolution** (adding/removing columns over time)
- **Use case fit** (streaming, analytics, data exchange)

---

### 2. Key Concepts: Row vs Column, Predicate Pushdown, Compression, Data Skipping

#### Row-based vs column-based storage

**Row-based storage**
- Data is stored **one row at a time**: all columns for a row are stored together, then the next row, and so on.
- **Example:** Row 1: `[id=1, name=Alice, city=NYC, age=30]` â†’ Row 2: `[id=2, name=Bob, city=LA, age=25]` â†’ â€¦
- **Good for:** Reading or writing **full rows** (e.g. one customer record, one event).
- **Used by:** Avro, CSV, traditional relational row stores.

**Column-based (columnar) storage**
- Data is stored **one column at a time**: all values for column A, then all values for column B, etc.
- **Example:** `id: [1,2,3,...]` then `name: [Alice,Bob,...]` then `city: [NYC,LA,...]` then `age: [30,25,...]`.
- **Good for:** Reading **a subset of columns** or aggregating one column (e.g. `SUM(amount)`, `WHERE region = 'US'`).
- **Used by:** Parquet, ORC.

---

#### Why row-based = faster writes, column-based = faster reads

**Row-based â†’ faster writes**
- **One row = one write unit.** To insert a record you append one contiguous block (all columns for that row).
- No need to touch every column file; minimal seeks and I/O per row.
- **Streaming / transactional workloads** often write full rows â†’ row-based is a natural fit (e.g. Avro, Kafka).

**Column-based â†’ faster reads (for analytics)**
- **One column = one read unit.** A query like `SELECT SUM(revenue) WHERE year = 2024` only needs:
  - The `revenue` column (and maybe `year` for filtering).
- You **skip entire columns** you donâ€™t need â†’ less I/O and better cache use.
- Same column values are stored together â†’ **better compression** (repeated patterns) and **vectorized execution**.
- **Analytical queries** often need few columns and many rows â†’ column-based wins (e.g. Parquet, ORC).

**Summary**

| Operation     | Row-based        | Column-based     |
|--------------|------------------|------------------|
| **Write one row** | âœ… Fast (one block) | âŒ Slower (update many column chunks) |
| **Read few columns** | âŒ Read full rows   | âœ… Read only those columns |
| **Aggregations**   | âŒ Scan full rows   | âœ… Scan only needed columns |

---

#### Predicate pushdown

**What it means**
- **Predicate pushdown** = pushing **filter conditions down** to the storage layer (file format / reader) so that:
  - Only **rows (or row groups) that can satisfy the filter** are read and passed to the engine.
- The query engine says: â€œI only need rows where `region = 'US'` and `year >= 2023`.â€  
  The file format (e.g. Parquet, ORC) uses **metadata (min/max, statistics)** and **skips** row groups or stripes that cannot contain such rows.

**Why it helps**
- Less data read from disk â†’ **faster queries** and lower I/O cost.
- Works best when the format stores **per-block (or per-stripe) statistics** (e.g. min/max per column). Parquet and ORC both support this; ORC is often cited as having strong predicate pushdown.

**Example**
- Query: `SELECT * FROM sales WHERE date = '2024-01-15'`.
- With predicate pushdown: the reader checks each Parquet row groupâ€™s (or ORC stripeâ€™s) min/max for `date`; if `'2024-01-15'` is not in that range, the **entire block is skipped** and not read.

---

#### Data skipping

**What it means**
- **Data skipping** = avoiding reading **data that cannot contribute to the result**, using **metadata** (statistics) stored with the data.
- Common metadata: **min/max** per column per block, **null counts**, sometimes **bloom filters**.
- The engine (or the format reader) uses this metadata to **skip files, row groups, or stripes** before reading actual column data.

**How data skipping differs from predicate pushdown**

| | Predicate pushdown | Data skipping |
|---|--------------------|----------------|
| **What it is** | A **technique**: sending the filter (predicate) *down* to the storage layer instead of applying it only in the engine. | An **outcome**: not reading data that canâ€™t match the query (skipping files, row groups, or stripes). |
| **Focus** | *Where* the filter is applied (at scan/read time, close to the data). | *What* we achieve (less I/O by skipping irrelevant data). |
| **Scope** | Specifically about **filters** (e.g. `WHERE`, join conditions) being pushed to the reader. | Can include **any** use of metadata to skip data: filters (via pushdown), **partition pruning**, **file listing** (e.g. skip by path), etc. |
| **Analogy** | â€œGive the storage layer the filter so it can decide what to read.â€ | â€œWe skipped 80% of row groupsâ€ â€” the result of that decision. |

So: **predicate pushdown** is the *mechanism* (push predicate to storage); **data skipping** is the *effect* (skip blocks/rows using metadata). Predicate pushdown is one way to get data skipping; data skipping can also come from partition pruning or other metadata-based skipping without a pushed predicate.

**How it relates in practice**
- Predicate pushdown is the **mechanism**: â€œpush the predicate down to the storage layer.â€
- Data skipping is the **effect**: â€œskip blocks/rows that donâ€™t match the predicate using metadata.â€
- So **predicate pushdown enables data skipping** when the format stores the right statistics (e.g. Parquet row group stats, ORC stripe stats).

**Example**
- Table partitioned by `date`, stored as Parquet with row group statistics.
- Query: `WHERE amount > 1000`.
- Engine uses **min/max of `amount`** in each row group: if `max(amount) < 1000` in a row group, that whole row group is **skipped** (data skipping via predicate pushdown).

---

#### Partition pruning

**What it means**
- **Partition pruning** = not reading **entire partitions** that cannot contain rows matching the query.
- Data is stored in a **partitioned layout** (e.g. by date, region): each partition is a separate directory or set of files (e.g. `year=2024/month=01/day=15/`).
- The engine looks at the query filter (e.g. `WHERE date = '2024-01-15'`) and **lists only the partition paths** that can match; it never opens other partition directories. Thatâ€™s partition pruning.

**How it works**
- **Partition columns** are usually in the path or in metadata (e.g. `date=2024-01-15`).
- Query has a predicate on the partition column â†’ engine converts it to a **list of partition paths** and only reads those.
- No need to open every file: **whole partitions are skipped** at the directory/metadata level, before reading any row data.

**Example**
- Layout: `s3://bucket/events/year=2024/month=01/day=01/`, `.../day=02/`, â€¦, `.../day=31/`.
- Query: `SELECT * FROM events WHERE year = 2024 AND month = 1 AND day = 15`.
- With partition pruning: only `.../year=2024/month=01/day=15/` is read; all other `day=*` directories are **skipped** (no listing, no scan).

**How it relates to predicate pushdown and data skipping**
- **Predicate pushdown** can push the partition-column filter down so the **catalog or file listing** only returns matching partition paths â†’ thatâ€™s partition pruning.
- **Data skipping** = â€œskip data that canâ€™t match.â€ Partition pruning is a form of data skipping: we skip **whole partitions** using partition metadata (path/metadata), not per-block stats. So:
  - **Partition pruning** = data skipping at the **partition** level (skip entire partition dirs).
  - **Predicate pushdown on non-partition columns** = data skipping at **row group / stripe** level (skip blocks within a partition).

**Best practice**
- Put frequently filtered columns (e.g. `date`, `region`) in the **partition key** so the engine can prune whole partitions and read far less data.

---

#### Compression types and what Snappy means

**Why compress**
- **Smaller files** â†’ less storage, less I/O, often **faster** reads (and sometimes writes) when the cost of decompression is less than the I/O saved.

**Common compression types**

| Compression   | Speed        | Ratio   | Typical use                          |
|---------------|-------------|--------|--------------------------------------|
| **None**      | Fastest     | 1x     | When I/O is cheap or data already small |
| **Snappy**    | Fast        | Good   | Default in Parquet; balance of speed and size |
| **GZIP**      | Slower      | Better | When storage matters more than CPU   |
| **LZ4**       | Very fast   | Good   | Low-latency, fast decompression      |
| **ZSTD**      | Configurable| Better| Newer format; good ratio and speed    |
| **Brotli**    | Slower      | High   | When max compression is desired      |

**What Snappy compression means**
- **Snappy** is a **fast** compression algorithm (by Google):
  - **Fast encode and decode** â†’ low CPU overhead, good for interactive and batch workloads.
  - **Moderate compression ratio** â†’ smaller than no compression, but not as small as GZIP or ZSTD at high levels.
- **Parquet** often uses Snappy by default so that:
  - Files are smaller than uncompressed and I/O is reduced.
  - Compression/decompression stays cheap so that **scan speed** and **query latency** remain good.
- Trade-off: Snappy favors **speed over maximum compression**; for colder data you can switch to GZIP or ZSTD for better ratio.

**In file formats**
- **Parquet:** Often Snappy by default; can use GZIP, LZ4, ZSTD, etc. per column.
- **ORC:** Uses internal compression (e.g. Zlib); typically high ratio.
- **Avro:** Supports codecs like Snappy, Deflate (GZIP); medium ratio, good for streaming.

---

### 3. Comparison Table: File Formats Overview

| File Format | Type | Best For | Supports Schema Evolution? | Compression |
|-------------|------|----------|---------------------------|-------------|
| **ORC** | Columnar | Apache Hive, ACID Transactions | âœ… Yes | âœ… High |
| **Parquet** | Columnar | Data Warehousing (Snowflake, Redshift, BigQuery) | âœ… Yes | âœ… High |
| **Avro** | Row-based | Streaming & Schema Evolution (Kafka, Spark, Flink) | âœ… Yes | âœ… Medium |
| **JSON** | Semi-Structured | APIs, Log Data | âœ… Yes | âŒ No (unless compressed) |
| **CSV** | Row-based | Simple Data Exchange | âŒ No | âŒ No |

---

### 4. ORC (Optimized Row Columnar)

**Best For**: Apache Hive, ACID Transactions in Hive

**Key Characteristics**:
- **Columnar Format**: Stores data in columns â†’ Faster query performance
- **Predicate Pushdown**: Only reads relevant data for queries
- **Highly Compressed**: Better compression than Parquet for Hive workloads
- **ACID Support**: Supports transactions, updates, and deletes in Hive
- **Metadata Storage**: Stores metadata for fast schema discovery

**âœ… Pros**:
- âœ”ï¸ Best for Hive tables (supports updates & deletes with ACID)
- âœ”ï¸ High compression & indexing â†’ Reduces storage & speeds up queries
- âœ”ï¸ Stores metadata (fast schema discovery)
- âœ”ï¸ More compression efficient than Parquet
- âœ”ï¸ Better predicate pushdown capabilities
- âœ”ï¸ Reduces NameNode load (single file per task)

**âŒ Cons**:
- âŒ Not as widely used as Parquet outside Hive
- âŒ Slower than Parquet for cloud-based analytics (Snowflake, BigQuery)
- âŒ Less capable of storing nested data compared to Parquet

**When to Use**:
- Working with Apache Hive
- Need ACID transaction support
- Hive-based data warehouses
- When predicate pushdown is critical

**Example Use Case**: Hive data warehouse with frequent updates and deletes

---

### 5. Parquet

**Best For**: Cloud-based Data Warehouses (Snowflake, Redshift, BigQuery, Databricks)

**Key Characteristics**:
- **Columnar Format**: Optimized for read-heavy workloads
- **Distributed File Systems**: Better for HDFS, S3, ADLS
- **Metadata Storage**: Stores file structure, schema, and column statistics
- **Compression**: Uses Snappy compression by default
- **Schema Evolution**: Supports schema append (adding columns)

**âœ… Pros**:
- âœ”ï¸ Faster queries in OLAP systems (BI, analytics)
- âœ”ï¸ Highly compressed (stores only relevant columns in queries)
- âœ”ï¸ Supports schema evolution (adding columns)
- âœ”ï¸ Better retrieval time compared to Avro
- âœ”ï¸ Better in-built functionality in Spark
- âœ”ï¸ More capable of storing nested data than ORC
- âœ”ï¸ Ideal for analytical querying (reads are much more efficient than writes)

**âŒ Cons**:
- âŒ Not the best for Hive ACID transactions (use ORC for that)
- âŒ Writes are slower compared to Avro
- âŒ Only supports schema append (not full schema evolution like Avro)
- âŒ Less compression efficient than ORC

**When to Use**:
- Cloud data warehouses (Snowflake, Redshift, BigQuery)
- Analytical workloads with large-scale data processing
- Querying a subset of columns in multi-column tables
- Read-heavy workloads

**Technical Details**:
- **Columnar Storage**: Values from the same column are stored together
- **Efficient Compression**: Columnar format enables efficient compression and encoding
- **Data Skipping**: Query engines can skip unnecessary data blocks based on statistics
- **Query Planning**: Metadata enables optimized query planning and execution

**Example Use Case**: Data warehouse analytics with frequent columnar queries

---

### 6. Avro

**Best For**: Streaming, Kafka, Schema Evolution

**Key Characteristics**:
- **Row-based Format**: Good for fast data writes
- **Schema Storage**: Stores schema with the data (JSON format)
- **Schema Evolution**: Robust support for schema changes over time
- **Random Access**: Supports random access despite compression
- **Backward & Forward Compatibility**: Handles schema changes gracefully

**âœ… Pros**:
- âœ”ï¸ Best for streaming & real-time pipelines (Kafka, Spark, Flink)
- âœ”ï¸ Supports comprehensive schema evolution (adding/modifying columns, missing fields)
- âœ”ï¸ Compressed format with random access support
- âœ”ï¸ Faster writes compared to Parquet
- âœ”ï¸ More mature schema evolution than Parquet
- âœ”ï¸ Ideal for ETL operations where all columns are queried
- âœ”ï¸ Schema stored in JSON format (easy to read and interpret)

**âŒ Cons**:
- âŒ Not optimized for analytics (use Parquet or ORC for that)
- âŒ Slower reads compared to Parquet
- âŒ Less compression than Parquet/ORC
- âŒ Not ideal for querying subset of columns

**When to Use**:
- Streaming data pipelines (Kafka, Kinesis)
- Real-time data processing
- When schema changes frequently
- ETL operations requiring all columns
- Data lake landing zones (write-heavy)

**Schema Evolution Features**:
- Handles missing fields
- Handles added fields
- Handles changed fields
- Backward compatibility (old readers can read new data)
- Forward compatibility (new readers can read old data)

**Example Use Case**: Kafka streaming pipeline with evolving schemas

---

### 7. JSON (JavaScript Object Notation)

**Best For**: Semi-Structured Data, API Logs

**Key Characteristics**:
- **Self-Describing**: Schema stored with data
- **Flexible Schema**: Not optimized for fast queries
- **Human-Readable**: Easy to read and write
- **Widely Supported**: Works with many systems

**âœ… Pros**:
- âœ”ï¸ Easy to read & write
- âœ”ï¸ Supports schema evolution (flexible structure)
- âœ”ï¸ Best for NoSQL & API data (MongoDB, Elasticsearch, Kafka, Cloud Storage)
- âœ”ï¸ Human-readable format
- âœ”ï¸ Self-describing (schema embedded in data)

**âŒ Cons**:
- âŒ Not efficient for analytics (slow queries)
- âŒ Takes more storage than columnar formats (Parquet, ORC)
- âŒ No built-in compression (unless compressed externally)
- âŒ Requires full file scan for queries
- âŒ No indexing support

**When to Use**:
- API responses and logs
- NoSQL databases (MongoDB, Elasticsearch)
- Data exchange between systems
- Semi-structured data
- Small to medium datasets

**Example Use Case**: API log storage, NoSQL database exports

---

### 8. CSV (Comma-Separated Values)

**Best For**: Small Datasets & Data Exchange

**Key Characteristics**:
- **Plain Text Format**: Separated by commas
- **Widely Supported**: Universal compatibility
- **No Indexing**: Lacks built-in indexing
- **No Compression**: Uncompressed format

**âœ… Pros**:
- âœ”ï¸ Simple & human-readable
- âœ”ï¸ Good for data exchange (Excel, Google Sheets, APIs)
- âœ”ï¸ Universal compatibility
- âœ”ï¸ Easy to import/export

**âŒ Cons**:
- âŒ Not efficient for big data processing (no compression)
- âŒ Slow queries (needs full file scan)
- âŒ No schema evolution support
- âŒ No compression
- âŒ No indexing
- âŒ Large file sizes

**When to Use**:
- Small datasets
- Data exchange between systems
- Excel/Google Sheets integration
- Simple data import/export
- Prototyping and testing

**Example Use Case**: Small data exports, Excel integration, simple data exchange

---

## ðŸ”„ File Format Comparisons

### Avro vs. Parquet

| Aspect | Avro | Parquet |
|--------|------|---------|
| **Storage Format** | Row-based | Columnar |
| **Best For** | Streaming, ETL (all columns) | Analytics, querying subset of columns |
| **Write Performance** | âœ… Faster writes | âŒ Slower writes |
| **Read Performance** | âŒ Slower reads | âœ… Faster reads |
| **Compression** | Medium | High |
| **Schema Evolution** | âœ… Comprehensive (add/modify columns, missing fields) | âš ï¸ Limited (schema append only) |
| **Query Performance** | âŒ Not optimized for analytics | âœ… Optimized for analytical querying |
| **Use Case** | ETL operations, streaming | Analytical workloads, OLAP |

**Key Differences**:
- **Avro**: Row-based, faster writes, better schema evolution, ideal for streaming and ETL
- **Parquet**: Columnar, faster reads, better compression, ideal for analytics and OLAP

**When to Choose**:
- **Choose Avro** when: Streaming data, frequent schema changes, ETL operations requiring all columns
- **Choose Parquet** when: Analytical queries, querying subset of columns, read-heavy workloads

---

### ORC vs. Parquet

| Aspect | ORC | Parquet |
|--------|-----|---------|
| **Storage Format** | Columnar | Columnar |
| **Best For** | Apache Hive, ACID Transactions | Cloud Data Warehouses |
| **Compression** | âœ… More compression efficient | âš ï¸ Less compression efficient |
| **ACID Support** | âœ… Yes (Hive) | âŒ No |
| **Nested Data** | âš ï¸ Less capable | âœ… More capable |
| **Predicate Pushdown** | âœ… Better | âš ï¸ Good |
| **Cloud Analytics** | âŒ Slower (Snowflake, BigQuery) | âœ… Faster |
| **Hive Performance** | âœ… Optimized for Hive | âš ï¸ Not optimized for Hive ACID |

**Key Differences**:
- **ORC**: Better compression, ACID support, better predicate pushdown, optimized for Hive
- **Parquet**: Better nested data support, faster in cloud analytics, more widely adopted

**When to Choose**:
- **Choose ORC** when: Working with Hive, need ACID transactions, Hive-based data warehouse
- **Choose Parquet** when: Cloud data warehouses (Snowflake, Redshift), need nested data support, cloud analytics

---

## ðŸ“‹ File Format Selection Guide

**Choose ORC when**:
- Using Apache Hive
- Need ACID transaction support
- Working with Hive-based data warehouses
- Predicate pushdown is critical

**Choose Parquet when**:
- Using cloud data warehouses (Snowflake, Redshift, BigQuery)
- Analytical workloads with read-heavy operations
- Querying subset of columns
- Need nested data support
- Cloud-based analytics

**Choose Avro when**:
- Streaming data pipelines (Kafka, Kinesis)
- Frequent schema changes
- ETL operations requiring all columns
- Write-heavy workloads
- Real-time data processing

**Choose JSON when**:
- API responses and logs
- NoSQL databases (MongoDB, Elasticsearch)
- Semi-structured data
- Data exchange between systems
- Small to medium datasets

**Choose CSV when**:
- Small datasets
- Simple data exchange
- Excel/Google Sheets integration
- Prototyping and testing
- Human-readable requirements

---

## ðŸ“Š Compression and Performance Summary

**File Formats with Compression**:
- âœ… **ORC**: High compression, best for Hive
- âœ… **Parquet**: High compression (Snappy by default), best for analytics
- âœ… **Avro**: Medium compression, best for streaming

**Performance Characteristics**:
- **Write Performance**: Avro > Parquet > ORC
- **Read Performance**: Parquet â‰ˆ ORC > Avro
- **Compression**: ORC > Parquet > Avro
- **Schema Evolution**: Avro > Parquet (append only) > ORC

---

## âœ… Check Your Understanding

1. When would you choose Parquet over Avro?
2. What is the main advantage of ORC over Parquet in a Hive environment?
3. Why is Avro better for streaming than Parquet?
4. What are the trade-offs between row-based and columnar formats?
5. Which format supports the most flexible schema evolution?

---

## ðŸŽ¯ Next Steps

Once you're comfortable with file formats, move on to:
- **Topic 4: Advanced SQL** (or next topic in your study plan)

**Study Time**: Spend 1â€“2 days on this topic, then practice choosing formats in design scenarios.

---

## ðŸ“š Additional Resources

- [Apache Parquet Format](https://parquet.apache.org/)
- [Apache Avro Documentation](https://avro.apache.org/)
- [ORC File Format](https://orc.apache.org/)
- [Delta Lake File Format](https://delta.io/) (built on Parquet)
