# Topic 3: Data File Formats

## ðŸŽ¯ Learning Goals

By the end of this topic, you should be able to:
- Understand key data file formats: ORC, Parquet, Avro, JSON, and CSV
- Choose the right format for storage, streaming, analytics, and exchange
- Compare columnar vs row-based formats and when to use each
- Explain schema evolution support across formats
- Apply compression and performance trade-offs in practice

---

## ðŸ“– Core Concepts

### 1. Why File Formats Matter

The choice of file format significantly impacts:
- **Storage efficiency** (compression, size)
- **Query performance** (read/write speed, predicate pushdown)
- **Schema evolution** (adding/removing columns over time)
- **Use case fit** (streaming, analytics, data exchange)

---

### 2. Comparison Table: File Formats Overview

| File Format | Type | Best For | Supports Schema Evolution? | Compression |
|-------------|------|----------|---------------------------|-------------|
| **ORC** | Columnar | Apache Hive, ACID Transactions | âœ… Yes | âœ… High |
| **Parquet** | Columnar | Data Warehousing (Snowflake, Redshift, BigQuery) | âœ… Yes | âœ… High |
| **Avro** | Row-based | Streaming & Schema Evolution (Kafka, Spark, Flink) | âœ… Yes | âœ… Medium |
| **JSON** | Semi-Structured | APIs, Log Data | âœ… Yes | âŒ No (unless compressed) |
| **CSV** | Row-based | Simple Data Exchange | âŒ No | âŒ No |

---

### 3. ORC (Optimized Row Columnar)

**Best For**: Apache Hive, ACID Transactions in Hive

**Key Characteristics**:
- **Columnar Format**: Stores data in columns â†’ Faster query performance
- **Predicate Pushdown**: Only reads relevant data for queries
- **Highly Compressed**: Better compression than Parquet for Hive workloads
- **ACID Support**: Supports transactions, updates, and deletes in Hive
- **Metadata Storage**: Stores metadata for fast schema discovery

**âœ… Pros**:
- âœ”ï¸ Best for Hive tables (supports updates & deletes with ACID)
- âœ”ï¸ High compression & indexing â†’ Reduces storage & speeds up queries
- âœ”ï¸ Stores metadata (fast schema discovery)
- âœ”ï¸ More compression efficient than Parquet
- âœ”ï¸ Better predicate pushdown capabilities
- âœ”ï¸ Reduces NameNode load (single file per task)

**âŒ Cons**:
- âŒ Not as widely used as Parquet outside Hive
- âŒ Slower than Parquet for cloud-based analytics (Snowflake, BigQuery)
- âŒ Less capable of storing nested data compared to Parquet

**When to Use**:
- Working with Apache Hive
- Need ACID transaction support
- Hive-based data warehouses
- When predicate pushdown is critical

**Example Use Case**: Hive data warehouse with frequent updates and deletes

---

### 4. Parquet

**Best For**: Cloud-based Data Warehouses (Snowflake, Redshift, BigQuery, Databricks)

**Key Characteristics**:
- **Columnar Format**: Optimized for read-heavy workloads
- **Distributed File Systems**: Better for HDFS, S3, ADLS
- **Metadata Storage**: Stores file structure, schema, and column statistics
- **Compression**: Uses Snappy compression by default
- **Schema Evolution**: Supports schema append (adding columns)

**âœ… Pros**:
- âœ”ï¸ Faster queries in OLAP systems (BI, analytics)
- âœ”ï¸ Highly compressed (stores only relevant columns in queries)
- âœ”ï¸ Supports schema evolution (adding columns)
- âœ”ï¸ Better retrieval time compared to Avro
- âœ”ï¸ Better in-built functionality in Spark
- âœ”ï¸ More capable of storing nested data than ORC
- âœ”ï¸ Ideal for analytical querying (reads are much more efficient than writes)

**âŒ Cons**:
- âŒ Not the best for Hive ACID transactions (use ORC for that)
- âŒ Writes are slower compared to Avro
- âŒ Only supports schema append (not full schema evolution like Avro)
- âŒ Less compression efficient than ORC

**When to Use**:
- Cloud data warehouses (Snowflake, Redshift, BigQuery)
- Analytical workloads with large-scale data processing
- Querying a subset of columns in multi-column tables
- Read-heavy workloads

**Technical Details**:
- **Columnar Storage**: Values from the same column are stored together
- **Efficient Compression**: Columnar format enables efficient compression and encoding
- **Data Skipping**: Query engines can skip unnecessary data blocks based on statistics
- **Query Planning**: Metadata enables optimized query planning and execution

**Example Use Case**: Data warehouse analytics with frequent columnar queries

---

### 5. Avro

**Best For**: Streaming, Kafka, Schema Evolution

**Key Characteristics**:
- **Row-based Format**: Good for fast data writes
- **Schema Storage**: Stores schema with the data (JSON format)
- **Schema Evolution**: Robust support for schema changes over time
- **Random Access**: Supports random access despite compression
- **Backward & Forward Compatibility**: Handles schema changes gracefully

**âœ… Pros**:
- âœ”ï¸ Best for streaming & real-time pipelines (Kafka, Spark, Flink)
- âœ”ï¸ Supports comprehensive schema evolution (adding/modifying columns, missing fields)
- âœ”ï¸ Compressed format with random access support
- âœ”ï¸ Faster writes compared to Parquet
- âœ”ï¸ More mature schema evolution than Parquet
- âœ”ï¸ Ideal for ETL operations where all columns are queried
- âœ”ï¸ Schema stored in JSON format (easy to read and interpret)

**âŒ Cons**:
- âŒ Not optimized for analytics (use Parquet or ORC for that)
- âŒ Slower reads compared to Parquet
- âŒ Less compression than Parquet/ORC
- âŒ Not ideal for querying subset of columns

**When to Use**:
- Streaming data pipelines (Kafka, Kinesis)
- Real-time data processing
- When schema changes frequently
- ETL operations requiring all columns
- Data lake landing zones (write-heavy)

**Schema Evolution Features**:
- Handles missing fields
- Handles added fields
- Handles changed fields
- Backward compatibility (old readers can read new data)
- Forward compatibility (new readers can read old data)

**Example Use Case**: Kafka streaming pipeline with evolving schemas

---

### 6. JSON (JavaScript Object Notation)

**Best For**: Semi-Structured Data, API Logs

**Key Characteristics**:
- **Self-Describing**: Schema stored with data
- **Flexible Schema**: Not optimized for fast queries
- **Human-Readable**: Easy to read and write
- **Widely Supported**: Works with many systems

**âœ… Pros**:
- âœ”ï¸ Easy to read & write
- âœ”ï¸ Supports schema evolution (flexible structure)
- âœ”ï¸ Best for NoSQL & API data (MongoDB, Elasticsearch, Kafka, Cloud Storage)
- âœ”ï¸ Human-readable format
- âœ”ï¸ Self-describing (schema embedded in data)

**âŒ Cons**:
- âŒ Not efficient for analytics (slow queries)
- âŒ Takes more storage than columnar formats (Parquet, ORC)
- âŒ No built-in compression (unless compressed externally)
- âŒ Requires full file scan for queries
- âŒ No indexing support

**When to Use**:
- API responses and logs
- NoSQL databases (MongoDB, Elasticsearch)
- Data exchange between systems
- Semi-structured data
- Small to medium datasets

**Example Use Case**: API log storage, NoSQL database exports

---

### 7. CSV (Comma-Separated Values)

**Best For**: Small Datasets & Data Exchange

**Key Characteristics**:
- **Plain Text Format**: Separated by commas
- **Widely Supported**: Universal compatibility
- **No Indexing**: Lacks built-in indexing
- **No Compression**: Uncompressed format

**âœ… Pros**:
- âœ”ï¸ Simple & human-readable
- âœ”ï¸ Good for data exchange (Excel, Google Sheets, APIs)
- âœ”ï¸ Universal compatibility
- âœ”ï¸ Easy to import/export

**âŒ Cons**:
- âŒ Not efficient for big data processing (no compression)
- âŒ Slow queries (needs full file scan)
- âŒ No schema evolution support
- âŒ No compression
- âŒ No indexing
- âŒ Large file sizes

**When to Use**:
- Small datasets
- Data exchange between systems
- Excel/Google Sheets integration
- Simple data import/export
- Prototyping and testing

**Example Use Case**: Small data exports, Excel integration, simple data exchange

---

## ðŸ”„ File Format Comparisons

### Avro vs. Parquet

| Aspect | Avro | Parquet |
|--------|------|---------|
| **Storage Format** | Row-based | Columnar |
| **Best For** | Streaming, ETL (all columns) | Analytics, querying subset of columns |
| **Write Performance** | âœ… Faster writes | âŒ Slower writes |
| **Read Performance** | âŒ Slower reads | âœ… Faster reads |
| **Compression** | Medium | High |
| **Schema Evolution** | âœ… Comprehensive (add/modify columns, missing fields) | âš ï¸ Limited (schema append only) |
| **Query Performance** | âŒ Not optimized for analytics | âœ… Optimized for analytical querying |
| **Use Case** | ETL operations, streaming | Analytical workloads, OLAP |

**Key Differences**:
- **Avro**: Row-based, faster writes, better schema evolution, ideal for streaming and ETL
- **Parquet**: Columnar, faster reads, better compression, ideal for analytics and OLAP

**When to Choose**:
- **Choose Avro** when: Streaming data, frequent schema changes, ETL operations requiring all columns
- **Choose Parquet** when: Analytical queries, querying subset of columns, read-heavy workloads

---

### ORC vs. Parquet

| Aspect | ORC | Parquet |
|--------|-----|---------|
| **Storage Format** | Columnar | Columnar |
| **Best For** | Apache Hive, ACID Transactions | Cloud Data Warehouses |
| **Compression** | âœ… More compression efficient | âš ï¸ Less compression efficient |
| **ACID Support** | âœ… Yes (Hive) | âŒ No |
| **Nested Data** | âš ï¸ Less capable | âœ… More capable |
| **Predicate Pushdown** | âœ… Better | âš ï¸ Good |
| **Cloud Analytics** | âŒ Slower (Snowflake, BigQuery) | âœ… Faster |
| **Hive Performance** | âœ… Optimized for Hive | âš ï¸ Not optimized for Hive ACID |

**Key Differences**:
- **ORC**: Better compression, ACID support, better predicate pushdown, optimized for Hive
- **Parquet**: Better nested data support, faster in cloud analytics, more widely adopted

**When to Choose**:
- **Choose ORC** when: Working with Hive, need ACID transactions, Hive-based data warehouse
- **Choose Parquet** when: Cloud data warehouses (Snowflake, Redshift), need nested data support, cloud analytics

---

## ðŸ“‹ File Format Selection Guide

**Choose ORC when**:
- Using Apache Hive
- Need ACID transaction support
- Working with Hive-based data warehouses
- Predicate pushdown is critical

**Choose Parquet when**:
- Using cloud data warehouses (Snowflake, Redshift, BigQuery)
- Analytical workloads with read-heavy operations
- Querying subset of columns
- Need nested data support
- Cloud-based analytics

**Choose Avro when**:
- Streaming data pipelines (Kafka, Kinesis)
- Frequent schema changes
- ETL operations requiring all columns
- Write-heavy workloads
- Real-time data processing

**Choose JSON when**:
- API responses and logs
- NoSQL databases (MongoDB, Elasticsearch)
- Semi-structured data
- Data exchange between systems
- Small to medium datasets

**Choose CSV when**:
- Small datasets
- Simple data exchange
- Excel/Google Sheets integration
- Prototyping and testing
- Human-readable requirements

---

## ðŸ“Š Compression and Performance Summary

**File Formats with Compression**:
- âœ… **ORC**: High compression, best for Hive
- âœ… **Parquet**: High compression (Snappy by default), best for analytics
- âœ… **Avro**: Medium compression, best for streaming

**Performance Characteristics**:
- **Write Performance**: Avro > Parquet > ORC
- **Read Performance**: Parquet â‰ˆ ORC > Avro
- **Compression**: ORC > Parquet > Avro
- **Schema Evolution**: Avro > Parquet (append only) > ORC

---

## âœ… Check Your Understanding

1. When would you choose Parquet over Avro?
2. What is the main advantage of ORC over Parquet in a Hive environment?
3. Why is Avro better for streaming than Parquet?
4. What are the trade-offs between row-based and columnar formats?
5. Which format supports the most flexible schema evolution?

---

## ðŸŽ¯ Next Steps

Once you're comfortable with file formats, move on to:
- **Topic 4: Advanced SQL** (or next topic in your study plan)

**Study Time**: Spend 1â€“2 days on this topic, then practice choosing formats in design scenarios.

---

## ðŸ“š Additional Resources

- [Apache Parquet Format](https://parquet.apache.org/)
- [Apache Avro Documentation](https://avro.apache.org/)
- [ORC File Format](https://orc.apache.org/)
- [Delta Lake File Format](https://delta.io/) (built on Parquet)
